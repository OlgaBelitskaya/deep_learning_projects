<!DOCTYPE HTML>
<html>
  <head>
    <meta charset='utf-8'>
    <meta name='viewport' content='width=device-width'>
    <title>üèôDLPP5SMC</title>
    <script src='https://sagecell.sagemath.org/static/embedded_sagecell.js'></script>
    <script>$(function(){sagecell.makeSagecell(
      {inputLocation:'div.linked',linked:true,evalButtonText:'Run Linked Cells'});});
    </script>
  </head>
  <style>
    @import 'https://fonts.googleapis.com/css?family=Akronim|Lobster';
    h1,h2,th {color:#348ABD; font-family:'Akronim'; font-size:120%; text-shadow:4px 4px 4px #aaa;}
    p,a {color:slategray; font-family:'Lobster'; font-size:120%; text-shadow:4px 4px 4px #aaa;}
    .sagecell .CodeMirror-scroll {min-height:3em; max-height:60em;}
    body {margin:5px 5px 5px 15px;}
  </style>
  <body>
    <h1>üìë &nbsp; Deep Learning. P5: Decor Recognition & Colorization</h1>
<a href='https://olgabelitskaya.github.io/README.html'>&#x1F300; &nbsp; Home Page &nbsp; &nbsp; &nbsp;</a>
<a href='https://colab.research.google.com/drive/1Tt3qZePsf2P6kNNao-hQ58DlG71Abj5a'>
&#x1F300; &nbsp; Google Colaboratory Variant &nbsp; &nbsp; &nbsp;</a>
<a href='https://www.instagram.com/olga.belitskaya/'>&#x1F300; &nbsp; Instagram Posts &nbsp; &nbsp; &nbsp;</a>     
<a href='https://www.pinterest.ru/olga_belitskaya/code-style/'>&#x1F300; &nbsp; Pinterest Posts</a><br/>
For this project, I have created the dataset of color images (150x150x3) with traditional patterns.<br/>
Let's have a look at some decor contours. 
<div class='linked'><script type='text/x-sage'>
import warnings; warnings.filterwarnings('ignore')
import pylab; from skimage import io,color,measure
@interact
def _vector(file=[1,2,3,4,5],
            cm=['ocean','cool','gnuplot2','terrain',
                'winter','spring','summer','autumn'],
            level=[round(.9-.03*i,2) for i in range(15)]):
    path1='https://olgabelitskaya.github.io/images/'
    path2='01_01_1_00%s'%(file)+'.png'
    img=io.imread(path1+path2); level=float(level)
    gray_img=color.colorconv.rgb2gray(
        color.colorconv.rgba2rgb(img)) 
    contours=measure.find_contours(gray_img,level)
    n=len(contours); pylab.figure(figsize=(5,5))
    pylab.gca().invert_yaxis()
    [pylab.plot(contours[i][:,1],contours[i][:,0],
                lw=.7,color=pylab.get_cmap(cm)(i/n)) 
     for i in range(n)]
    pylab.xticks([]); pylab.yticks([]); pylab.show()
</script></div><br/> 
    <h2>‚úíÔ∏è &nbsp;Step 0. Import Libraries</h2>      
<div class='linked'><script type='text/x-sage'>
!python3 -m pip install --upgrade pip \
--user --quiet --no-warn-script-location
!python3 -m pip install argon2-cffi \
--user --quiet --no-warn-script-location
!python3 -m pip install --ignore-installed --upgrade tensorflow==2.3.0 \
--user --quiet --no-warn-script-location
!python3 -m pip install h5py==2.10.0 \
--user --quiet --no-warn-script-location
path='/home/sc_work/.sage/local/lib/python3.8/site-packages'
import sys; sys.path.append(path)
import h5py,urllib,zipfile
import pandas as pd,numpy as np,pylab as pl
import seaborn as sn,tensorflow as tf
from skimage.transform import resize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
np.set_printoptions(precision=6)
pl.style.use('seaborn-whitegrid')
from tensorflow.keras.callbacks import \
ModelCheckpoint,EarlyStopping,ReduceLROnPlateau
from tensorflow.keras.models import Sequential,load_model,Model
from tensorflow.keras.layers import \
Input,Activation,Dense,LSTM,PReLU,LeakyReLU,\
Flatten,Dropout,BatchNormalization,\
Conv2D,MaxPooling2D,GlobalMaxPooling2D
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
fw='/tmp/checkpoint'
dr2,dr25,dr3,dr5=\
float(.2),float(.25),float(.3),float(.5)
fr,al=float(.5),float(.02)
[i0,i1,i2,i3,i4,i5,i6]=\
[int(0),int(1),int(2),int(3),int(4),int(5),int(6)]
[i7,i8,i10,i11,i12,i16]=\
[int(7),int(8),int(10),int(11),int(12),int(16)]
[i20,i24,i32,i48,i50,i64,i96]=\
[int(20),int(24),int(32),int(48),int(50),int(64),int(96)]
[i100,i128,i150,i196,i200]=\
[int(100),int(128),int(150),int(196),int(200)]
[i256,i512,i1024]=[int(256),int(512),int(1024)]
</script></div><br/>  
<div class='linked'><script type='text/x-sage'>
def ohe(x): 
    return OneHotEncoder(categories='auto').fit(x.reshape(-i1,i1))\
           .transform(x.reshape(-i1,i1)).toarray().astype('int64')
def tts(X,y): 
    x_train,x_test,y_train,y_test=\
    train_test_split(X,y,test_size=float(.2),random_state=i1)
    n=int(len(x_test)/2)
    x_valid,y_valid=x_test[:n],y_test[:n]
    x_test,y_test=x_test[n:],y_test[n:]
    return x_train,x_valid,x_test,y_train,y_valid,y_test
def resh(x,img_size):
    y=[resize(el,(img_size,img_size,i3),
              anti_aliasing=True) for el in x]
    return np.array(y)
def gresh(x,img_size): 
    y=np.array([resize(el,(img_size,img_size,int(1)),
                       anti_aliasing=True) for el in x])
    return y.reshape(-int(1),img_size,img_size,int(1))
def keras_history_plot(fit_history,fig_size,color):
    keys=list(fit_history.history.keys())
    list_history=[fit_history.history[keys[i]] 
                  for i in range(len(keys))]
    dfkeys=pd.DataFrame(list_history).T
    dfkeys.columns=keys
    fig=pl.figure(figsize=(fig_size,fig_size))
    ax1=fig.add_subplot(2,1,1)
    dfkeys.iloc[:,[int(0),int(2)]].plot(
        ax=ax1,color=['slategray',color],grid=True)
    ax2=fig.add_subplot(2,1,2)
    dfkeys.iloc[:,[int(1),int(3)]].plot(
        ax=ax2,color=['slategray',color],grid=True)
    pl.tight_layout(); pl.show()
</script></div><br/>
    <h2>‚úíÔ∏è &nbsp;Step 1. Load and Explore the Data</h2>
<div class='linked'><script type='text/x-sage'>
fpath='https://olgabelitskaya.github.io/'
zf='DecorColorImages.h5.zip'
input_file=urllib.request.urlopen(fpath+zf)
output_file=open(zf,'wb')
output_file.write(input_file.read())
output_file.close(); input_file.close()
zipf=zipfile.ZipFile(zf,'r')
zipf.extractall(''); zipf.close()
f=h5py.File(zf[:-4],'r'); keys=list(f.keys())
[countries,decors,images,types]=\
[np.array(f[keys[i]]) for i in range(4)]
pd.DataFrame([el.shape for el in 
              [countries,decors,images,types]])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
fpath2='https://raw.githubusercontent.com/OlgaBelitskaya/'+\
       'deep_learning_projects/master/DL_PP5/'
data=pd.read_csv(fpath2+'decor.txt')
n=np.random.choice(484,size=6,replace=False)
data.loc[n]
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
images=images/255
fig=pl.figure(figsize=(8,4))
for i,idx in enumerate(n):
    ax=fig.add_subplot(2,3,i+1,xticks=[],yticks=[])
    ax.imshow(images[idx])
    ax.set_title(
        data['country'][idx]+'; '+data['decor'][idx]+\
        '; '+data['type'][idx])
pl.tight_layout(); pl.show()
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
gray_images=np.dot(images[...,:3],[.299,.587,.114])
pl.figure(figsize=(3,3))
n=np.random.choice(484,size=1,replace=False)[0]
pl.imshow(images[n])
pl.title(data['country'][n]+'; '+\
         data['decor'][n]+'; '+data['type'][n])
pl.imshow(gray_images[n],cmap=pl.cm.bone)
pl.tight_layout(); pl.show()
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
ccountries,cdecors,ctypes=\
ohe(countries),ohe(decors),ohe(types)
ctargets=np.concatenate((ccountries,cdecors),axis=1)
ctargets=np.concatenate((ctargets,ctypes),axis=1)
images=resh(images,i48)
gray_images=gresh(gray_images,i64)
pd.DataFrame([images.shape,gray_images.shape,
              ccountries.shape,cdecors.shape,
              ctypes.shape,ctargets.shape])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
# Color Images / Countries 
x_train1,x_valid1,x_test1,\
y_train1,y_valid1,y_test1=tts(images,ccountries)
# Grayscaled Images / Countries 
x_train2,x_valid2,x_test2,\
y_train2,y_valid2,y_test2=tts(gray_images,ccountries)
# Color Images / Decors 
x_train3,x_valid3,x_test3,\
y_train3,y_valid3,y_test3=tts(images,cdecors)
# Grayscaled Images / Decors 
x_train4,x_valid4,x_test4,\
y_train4,y_valid4,y_test4=tts(gray_images,cdecors)
# Color Images / Multi-Label Targets
x_train5,x_valid5,x_test5,\
y_train5,y_valid5,y_test5=tts(images,ctargets)
# Grayscaled Images / Multi-Label Targets 
x_train6,x_valid6,x_test6,\
y_train6,y_valid6,y_test6=tts(gray_images,ctargets)
sh=[el.shape for el in \
[x_train1,y_train1,x_valid1,y_valid1,x_test1,y_test1,
 x_train3,y_train3,x_valid3,y_valid3,x_test3,y_test3,
 x_train5,y_train5,x_valid5,y_valid5,x_test5,y_test5]]
sh2=[el.shape for el in \
[x_train2,y_train2,x_valid2,y_valid2,x_test2,y_test2,
 x_train4,y_train4,x_valid4,y_valid4,x_test4,y_test4,
 x_train6,y_train6,x_valid6,y_valid6,x_test6,y_test6]]
pd.DataFrame([sh,sh2]).T
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
y_train5_list=[y_train5[:,:i4],y_train5[:,i4:i11], y_train5[:,i11:]]
y_test5_list=[y_test5[:,:i4],y_test5[:,i4:i11], y_test5[:,i11:]]
y_valid5_list=[y_valid5[:,:i4],y_valid5[:,i4:i11], y_valid5[:,i11:]]
y_train6_list=[y_train6[:,:i4],y_train6[:,i4:i11], y_train6[:,i11:]]
y_test6_list=[y_test6[:,:i4],y_test6[:,i4:i11], y_test6[:,i11:]]
y_valid6_list=[y_valid6[:,:i4],y_valid6[:,i4:i11], y_valid6[:,i11:]]
del images,gray_images,countries,decors,types
del ccountries,cdecors,ctypes,ctargets
</script></div><br/>
<p>Unfortunately, we can train with guarantee only one neural network at ones.</p>
<p>Please restart the page and load the data again for training each of them.</p>
    <h2>‚úíÔ∏è&nbsp;Step 2. One-Label Classification Models</h2>
<div class='linked'><script type='text/x-sage'>
# Color Images / Countries
def model():
    model=Sequential()
    model.add(Conv2D(i32,(i5,i5),padding='same',
                     input_shape=x_train1.shape[i1:]))
    model.add(Activation('relu'))    
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))
    model.add(Conv2D(i96,(i5,i5)))
    model.add(Activation('relu'))    
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))   
    model.add(GlobalMaxPooling2D())    
    model.add(Dense(i512,activation='relu'))
    model.add(Dropout(dr25))    
    model.add(Dense(i4))
    model.add(Activation('softmax'))   
    model.compile(loss='categorical_crossentropy',
                  optimizer='nadam',metrics=['accuracy'])   
    return model
model=model() 
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
del x_train4,x_valid4,x_test4,y_train4,y_valid4,y_test4,\
x_train2,x_valid2,x_test2,y_train2,y_valid2,y_test2,\
x_train3,x_valid3,x_test3,y_train3,y_valid3,y_test3,\
x_train5,x_valid5,x_test5,y_train5,y_valid5,y_test5,\
x_train6,x_valid6,x_test6,y_train6,y_valid6,y_test6
checkpointer=ModelCheckpoint(
    filepath=fw,verbose=i0,save_weights_only=True,
    monitor='val_accuracy',mode='max',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=i5,verbose=i0,factor=fr)
history=model.fit(x_train1,y_train1,epochs=i16,
                  batch_size=i16,verbose=i2,
                  validation_data=(x_valid1,y_valid1),
                  callbacks=[checkpointer,lr_reduction])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
keras_history_plot(history,7,'#348ABD')
model.load_weights(fw)
pretty_print(model.evaluate(x_test1,y_test1,verbose=i0))
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
# Color Images / Decors
def model(leaky_alpha):
    model=Sequential()
    model.add(Conv2D(i32,(i5,i5),padding='same', 
                     input_shape=x_train3.shape[i1:]))
    model.add(LeakyReLU(alpha=leaky_alpha))    
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))
    model.add(Conv2D(i96,(i5,i5)))
    model.add(LeakyReLU(alpha=leaky_alpha))    
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))   
    model.add(GlobalMaxPooling2D())     
    model.add(Dense(i512))
    model.add(LeakyReLU(alpha=leaky_alpha))
    model.add(Dropout(dr25))     
    model.add(Dense(i7))
    model.add(Activation('softmax'))   
    model.compile(loss='categorical_crossentropy',
                  optimizer='nadam',metrics=['accuracy'])   
    return model
model=model(float(.005))   
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
del x_train1,x_valid1,x_test1,y_train1,y_valid1,y_test1,\
x_train2,x_valid2,x_test2,y_train2,y_valid2,y_test2,\
x_train4,x_valid4,x_test4,y_train4,y_valid4,y_test4,\
x_train5,x_valid5,x_test5,y_train5,y_valid5,y_test5,\
x_train6,x_valid6,x_test6,y_train6,y_valid6,y_test6
checkpointer=ModelCheckpoint(
    filepath=fw,verbose=i0,save_weights_only=True,
    monitor='val_accuracy',mode='max',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=i5,verbose=i0,factor=fr)
history=model.fit(x_train3,y_train3, 
                  epochs=i16,batch_size=i16,verbose=i2,
                  validation_data=(x_valid3,y_valid3),
                  callbacks=[checkpointer,lr_reduction])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
keras_history_plot(history,7,'#348ABD')
model.load_weights(fw)
pretty_print(model.evaluate(x_test3,y_test3,verbose=i0))
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
# Grayscaled Images / Countries
def gray_model():
    model=Sequential()
    model.add(Conv2D(i16,(i5,i5),padding='same', 
                     input_shape=x_train2.shape[1:]))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))
    model.add(Conv2D(i128,(i5,i5)))
    model.add(Activation('relu'))    
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))   
    model.add(GlobalMaxPooling2D())    
    model.add(Dense(i512,activation='tanh'))
    model.add(Dropout(dr25))
    model.add(Dense(i4))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',metrics=['accuracy'])
    return model
gray_model=gray_model()
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
del x_train1,x_valid1,x_test1,y_train1,y_valid1,y_test1,\
x_train4,x_valid4,x_test4,y_train4,y_valid4,y_test4,\
x_train3,x_valid3,x_test3,y_train3,y_valid3,y_test3,\
x_train5,x_valid5,x_test5,y_train5,y_valid5,y_test5,\
x_train6,x_valid6,x_test6,y_train6,y_valid6,y_test6
checkpointer=ModelCheckpoint(
    filepath=fw,verbose=i0,save_weights_only=True,
    monitor='val_accuracy',mode='max',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=i5,verbose=i0,factor=fr)
history=gray_model.fit(x_train2,y_train2, 
                       epochs=i16,batch_size=i16,verbose=i2,
                       validation_data=(x_valid2,y_valid2),
                       callbacks=[checkpointer,lr_reduction])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
keras_history_plot(history,7,'#348ABD')
gray_model.load_weights(fw)
pretty_print(gray_model.evaluate(x_test2,y_test2,verbose=i0))
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
# Grayscaled Images / Decors
def gray_model(leaky_alpha):
    model=Sequential()
    model.add(Conv2D(i16,(i5,i5),padding='same', 
                     input_shape=x_train4.shape[i1:]))
    model.add(LeakyReLU(alpha=leaky_alpha))    
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))
    model.add(Conv2D(i128,(i5,i5)))
    model.add(LeakyReLU(alpha=leaky_alpha))    
    model.add(MaxPooling2D(pool_size=(i2,i2)))
    model.add(Dropout(dr25))  
    model.add(GlobalMaxPooling2D())    
    model.add(Dense(i512, activation='tanh'))
    model.add(Dropout(dr25))   
    model.add(Dense(i7))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',metrics=['accuracy'])
    return model
gray_model=gray_model(float(.01))
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
del x_train1,x_valid1,x_test1,y_train1,y_valid1,y_test1,\
x_train2,x_valid2,x_test2,y_train2,y_valid2,y_test2,\
x_train3,x_valid3,x_test3,y_train3,y_valid3,y_test3,\
x_train5,x_valid5,x_test5,y_train5,y_valid5,y_test5,\
x_train6,x_valid6,x_test6,y_train6,y_valid6,y_test6
checkpointer=ModelCheckpoint(
    filepath=fw,verbose=i0,save_weights_only=True,
    monitor='val_accuracy',mode='max',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=i5,verbose=i0,factor=fr)
history=gray_model.fit(x_train4,y_train4, 
                       epochs=i16,batch_size=i16,verbose=i2,
                       validation_data=(x_valid4,y_valid4),
                       callbacks=[checkpointer,lr_reduction])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
keras_history_plot(history,7,'#348ABD')
gray_model.load_weights(fw)
pretty_print(gray_model.evaluate(x_test4,y_test4,verbose=i0))
</script></div><br/>
    <h2>‚úíÔ∏è&nbsp;Step 3. Multi-Label Classification Models</h2>      
<div class='linked'><script type='text/x-sage'>
def multi_model(leaky_alpha):    
    model_input=Input(shape=x_train5.shape[i1:])
    x=BatchNormalization()(model_input)
    x=Conv2D(i32,(i5,i5),padding='same')(model_input)
    x=LeakyReLU(alpha=leaky_alpha)(x)
    x=MaxPooling2D(pool_size=(i2,i2))(x)    
    x=Dropout(dr25)(x)   
    x=Conv2D(i128,(i5,i5),padding='same')(x)
    x=LeakyReLU(alpha=leaky_alpha)(x)
    x=MaxPooling2D(pool_size=(i2,i2))(x)    
    x=Dropout(dr25)(x)            
    x=GlobalMaxPooling2D()(x)  
    x=Dense(i512)(x) 
    x=LeakyReLU(alpha=leaky_alpha)(x)
    x=Dropout(dr25)(x)    
    y1=Dense(i4,activation='softmax')(x)
    y2=Dense(i7,activation='softmax')(x)
    y3=Dense(i2,activation='softmax')(x)   
    model=Model(inputs=model_input,outputs=[y1,y2,y3])
    model.compile(loss='categorical_crossentropy',
                  optimizer='nadam',metrics=['accuracy'])   
    return model
multi_model=multi_model(float(.005))
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
del x_train1,x_valid1,x_test1,y_train1,y_valid1,y_test1,\
x_train2,x_valid2,x_test2,y_train2,y_valid2,y_test2,\
x_train3,x_valid3,x_test3,y_train3,y_valid3,y_test3,\
x_train4,x_valid4,x_test4,y_train4,y_valid4,y_test4,\
x_train6,x_valid6,x_test6,y_train6,y_valid6,y_test6
checkpointer=ModelCheckpoint(
    filepath=fw,verbose=i0,save_weights_only=True,
    monitor='val_loss',mode='min',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=i5,verbose=i0,factor=fr)
history=multi_model.fit(x_train5,y_train5_list, 
                        epochs=i12,batch_size=i16,verbose=i2,
                        validation_data=(x_valid5,y_valid5_list),
                        callbacks=[checkpointer,lr_reduction])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
multi_model.load_weights(fw)
multi_scores=multi_model.evaluate(x_test5,y_test5_list,verbose=i0)
print('Scores: \n' ,(multi_scores))
print('Country label. Accuracy: %.2f%%'%(multi_scores[i4]*100))
print('Decor label. Accuracy: %.2f%%'%(multi_scores[i5]*100))
print('Type label. Accuracy: %.2f%%'%(multi_scores[i6]*100))
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
def gray_multi_model(leaky_alpha):    
    model_input=Input(shape=x_train6.shape[i1:])
    x=BatchNormalization()(model_input)
    x=Conv2D(i32,(i5,i5), padding='same')(model_input)
    x=LeakyReLU(alpha=leaky_alpha)(x)
    x=MaxPooling2D(pool_size=(i2,i2))(x)    
    x=Dropout(dr25)(x)  
    x=Conv2D(i128,(i5,i5),padding='same')(x)
    x=LeakyReLU(alpha=leaky_alpha)(x)
    x=MaxPooling2D(pool_size=(i2,i2))(x)    
    x=Dropout(dr25)(x)             
    x=GlobalMaxPooling2D()(x) 
    x=Dense(i512)(x)
    x=LeakyReLU(alpha=leaky_alpha)(x)
    x=Dropout(dr25)(x)   
    y1=Dense(i4,activation='softmax')(x)
    y2=Dense(i7,activation='softmax')(x)
    y3=Dense(i2,activation='softmax')(x) 
    model=Model(inputs=model_input,outputs=[y1,y2,y3])   
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',metrics=['accuracy'])  
    return model
gray_multi_model=gray_multi_model(float(.01))
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
del x_train1,x_valid1,x_test1,y_train1,y_valid1,y_test1,\
x_train2,x_valid2,x_test2,y_train2,y_valid2,y_test2,\
x_train3,x_valid3,x_test3,y_train3,y_valid3,y_test3,\
x_train4,x_valid4,x_test4,y_train4,y_valid4,y_test4,\
x_train5,x_valid5,x_test5,y_train5,y_valid5,y_test5
checkpointer=ModelCheckpoint(
    filepath=fw,verbose=i0,save_weights_only=True,
    monitor='val_loss',mode='min',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=i5,verbose=i0,factor=fr)
history=gray_multi_model.fit(x_train6,y_train6_list,
                             epochs=i12,batch_size=i16,verbose=i2,
                             validation_data=(x_valid6,y_valid6_list),
                             callbacks=[checkpointer,lr_reduction])
</script></div><br/>
<div class='linked'><script type='text/x-sage'>
gray_multi_model.load_weights(fw)
gray_multi_scores=gray_multi_model.evaluate(x_test6,y_test6_list,verbose=i0)
print('Scores: \n',(gray_multi_scores))
print('Country label. Accuracy: %.2f%%'%(gray_multi_scores[i4]*100))
print('Decor label. Accuracy: %.2f%%'%(gray_multi_scores[i5]*100))
print('Type label. Accuracy: %.2f%%'%(gray_multi_scores[i6]*100))
</script></div><br/>
  </body>
</html> 